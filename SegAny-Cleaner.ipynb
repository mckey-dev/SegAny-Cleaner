{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999b108f-62c5-4c95-89fb-dd4f590f33cf",
   "metadata": {},
   "source": [
    "# Segment Anything(SAM) Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba69a3-ca79-48c7-9a93-701c85cc7ed8",
   "metadata": {},
   "source": [
    "## 仮想環境の作成とセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a6ef0-8705-46f7-b103-b7a32283adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 仮想環境のパス設定\n",
    "venv_path = \"/tmp/sac_env\"\n",
    "model_cache_dir = \"/tmp/models\"\n",
    "\n",
    "# モデル保存用のディレクトリ作成\n",
    "os.makedirs(model_cache_dir, exist_ok=True)\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = model_cache_dir\n",
    "\n",
    "# 仮想環境の作成とライブラリインストール\n",
    "if not os.path.exists(venv_path):\n",
    "    print(\"Creating virtual environment...\")\n",
    "    !python3 -m venv {venv_path}\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "!{venv_path}/bin/pip install --upgrade pip\n",
    "\n",
    "# PaperspaceのCUDA環境に合わせたPyTorchを明示的にインストール\n",
    "!{venv_path}/bin/pip install torch torchvision torchaudio\n",
    "!{venv_path}/bin/pip install gradio==3.41.2 opencv-python numpy Pillow\n",
    "\n",
    "# SAM2本体\n",
    "!{venv_path}/bin/pip install git+https://github.com/facebookresearch/segment-anything-2.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f641d-b49e-4d5b-9437-f606fc8347ce",
   "metadata": {},
   "source": [
    "## アプリケーションの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edf6a2-d796-4ab1-b7ab-71d5a441f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sac.py\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import gradio as gr\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# --- アプリケーション情報 ---\n",
    "# バグ修正とUI微調整を反映したVer 1.1.0\n",
    "VERSION = \"1.1.0\"\n",
    "\n",
    "# --- 環境設定 ---\n",
    "os.environ['MPLBACKEND'] = 'Agg'\n",
    "WORKDIR = \"/tmp/sac\"\n",
    "MODEL_DIR = \"/tmp/sac/models\"\n",
    "for d in [WORKDIR, MODEL_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "except ImportError:\n",
    "    print(\"SAM2 library not found.\")\n",
    "\n",
    "# --- カスタムCSS ---\n",
    "css = \"\"\"\n",
    ".padded-image { height: 600px !important; }\n",
    ".padded-image .image-container { padding-top: 50px !important; height: calc(100% - 50px) !important; }\n",
    ".padded-image img { max-height: 100% !important; object-fit: contain !important; }\n",
    ".version-text { text-align: center; color: gray; font-size: 1.6em; margin-top: 20px; font-weight: bold; }\n",
    "\"\"\"\n",
    "\n",
    "class SAM2Handler:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = None\n",
    "        self.current_model = None\n",
    "        self.active_image_path = os.path.join(WORKDIR, \"original.png\")\n",
    "        self.current_filename = \"result.png\"\n",
    "\n",
    "    def load_model(self, model_name, p_side, iou_th, stab_th, min_area):\n",
    "        target_path = os.path.join(MODEL_DIR, model_name)\n",
    "        params = (p_side, iou_th, stab_th, min_area)\n",
    "        if self.current_model == model_name and hasattr(self, 'current_params') and self.current_params == params: return\n",
    "        urls = {\n",
    "            \"sam2_hiera_tiny.pt\": (\"sam2_hiera_t.yaml\", \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\"),\n",
    "            \"sam2_hiera_small.pt\": (\"sam2_hiera_s.yaml\", \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt\"),\n",
    "            \"sam2_hiera_base_plus.pt\": (\"sam2_hiera_b+.yaml\", \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"),\n",
    "            \"sam2_hiera_large.pt\": (\"sam2_hiera_l.yaml\", \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\")\n",
    "        }\n",
    "        cfg, url = urls[model_name]\n",
    "        if not os.path.exists(target_path): urllib.request.urlretrieve(url, target_path)\n",
    "        model = build_sam2(cfg, target_path, device=self.device)\n",
    "        self.generator = SAM2AutomaticMaskGenerator(model=model, points_per_side=int(p_side), pred_iou_thresh=float(iou_th), stability_score_thresh=float(stab_th), crop_n_layers=1, min_mask_region_area=int(min_area))\n",
    "        self.current_model = model_name\n",
    "        self.current_params = params\n",
    "\n",
    "    def handle_upload(self, file_obj):\n",
    "        if not file_obj: return None\n",
    "        # 拡張子を強制的に .png に変更\n",
    "        base_name = os.path.splitext(os.path.basename(file_obj.name))[0]\n",
    "        self.current_filename = f\"{base_name}.png\"\n",
    "        shutil.copy(file_obj.name, self.active_image_path)\n",
    "        return Image.open(self.active_image_path)\n",
    "\n",
    "    def run_segment(self, model_name, p_side, iou_th, stab_th, min_area):\n",
    "        if not os.path.exists(self.active_image_path): return None\n",
    "        self.load_model(model_name, p_side, iou_th, stab_th, min_area)\n",
    "        orig_pil = Image.open(self.active_image_path).convert(\"RGB\")\n",
    "        img_np = np.array(orig_pil)\n",
    "        \n",
    "        try:\n",
    "            masks = self.generator.generate(img_np)\n",
    "            if not masks:\n",
    "                print(\"Warning: No masks detected.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"SAM2 Segmentation Error: {e}\")\n",
    "            return None\n",
    "\n",
    "        overlay = np.zeros_like(img_np)\n",
    "        for i, m in enumerate(masks):\n",
    "            color = [(i*47)%230+20, (i*97)%230+20, (i*149)%230+20]\n",
    "            overlay[m['segmentation']] = color\n",
    "        seg_pil = Image.fromarray(overlay)\n",
    "        seg_pil.save(os.path.join(WORKDIR, \"segments.png\"))\n",
    "        blended = Image.blend(seg_pil, orig_pil, 0.4)\n",
    "        return blended\n",
    "\n",
    "    def make_mask(self, sketch_data, invert, smooth, expand):\n",
    "        if sketch_data is None: return None\n",
    "        draw_img = sketch_data.get(\"mask\") if isinstance(sketch_data, dict) else sketch_data\n",
    "        seg_file = os.path.join(WORKDIR, \"segments.png\")\n",
    "        if draw_img is None or not os.path.exists(seg_file): return None\n",
    "        seg_np = np.array(Image.open(seg_file))\n",
    "        draw_np = np.array(draw_img.convert(\"L\"))\n",
    "        y, x = np.where(draw_np > 0)\n",
    "        if len(y) == 0: return None\n",
    "        target_colors = set(tuple(seg_np[y_i, x_i]) for y_i, x_i in zip(y, x))\n",
    "        target_colors.discard((0,0,0))\n",
    "        mask = np.zeros(seg_np.shape[:2], dtype=np.uint8)\n",
    "        for c in target_colors: mask[np.all(seg_np == c, axis=-1)] = 255\n",
    "        \n",
    "        if smooth > 0:\n",
    "            s_val = int(smooth) * 2 + 1\n",
    "            mask = cv2.GaussianBlur(mask, (s_val, s_val), 0); _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "        if expand != 0:\n",
    "            e_val = int(abs(expand))\n",
    "            k = np.ones((e_val, e_val), np.uint8)\n",
    "            mask = cv2.dilate(mask, k) if expand > 0 else cv2.erode(mask, k)\n",
    "            \n",
    "        if invert: mask = cv2.bitwise_not(mask)\n",
    "        mask_pil = Image.fromarray(mask).convert(\"RGB\")\n",
    "        mask_pil.save(os.path.join(WORKDIR, \"mask_generated.png\"))\n",
    "        orig_img = Image.open(self.active_image_path).convert(\"RGB\")\n",
    "        blended = Image.blend(mask_pil, orig_img, 0.5)\n",
    "        return blended\n",
    "\n",
    "    def run_crop(self, edited_mask_data, final_invert, crop_blur):\n",
    "        if not edited_mask_data or not os.path.exists(self.active_image_path): return None, None\n",
    "        gen_mask_path = os.path.join(WORKDIR, \"mask_generated.png\")\n",
    "        if not os.path.exists(gen_mask_path): return None, None\n",
    "        gen_mask_np = np.array(Image.open(gen_mask_path).convert(\"L\"))\n",
    "        if isinstance(edited_mask_data, dict) and \"mask\" in edited_mask_data:\n",
    "            stroke_np = np.array(edited_mask_data[\"mask\"].convert(\"L\"))\n",
    "            final_mask_np = cv2.subtract(gen_mask_np, stroke_np)\n",
    "        else:\n",
    "            final_mask_np = gen_mask_np\n",
    "        if final_invert:\n",
    "            final_mask_np = cv2.bitwise_not(final_mask_np)\n",
    "\n",
    "        if crop_blur > 0:\n",
    "            b_val = int(crop_blur) * 2 + 1\n",
    "            final_mask_np = cv2.GaussianBlur(final_mask_np, (b_val, b_val), 0)\n",
    "\n",
    "        Image.fromarray(final_mask_np).save(os.path.join(WORKDIR, \"mask_edited.png\"))\n",
    "        orig = np.array(Image.open(self.active_image_path).convert(\"RGB\"))\n",
    "        if orig.shape[:2] != final_mask_np.shape:\n",
    "            final_mask_np = cv2.resize(final_mask_np, (orig.shape[1], orig.shape[0]))\n",
    "        \n",
    "        # RGBAで透過処理\n",
    "        res_rgba = cv2.cvtColor(orig, cv2.COLOR_RGB2RGBA)\n",
    "        res_rgba[:, :, 3] = final_mask_np \n",
    "\n",
    "        out_path = os.path.join(WORKDIR, self.current_filename)\n",
    "        # 保存形式をPNGに強制（RGBA維持のため）\n",
    "        Image.fromarray(res_rgba).save(out_path, format=\"PNG\")\n",
    "        return Image.fromarray(res_rgba), out_path\n",
    "\n",
    "handler = SAM2Handler()\n",
    "version_info = f\"torch: {torch.__version__} | torchvision: {torchvision.__version__} | torchaudio: {torchaudio.__version__} | gradio: {gr.__version__}\"\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    gr.Markdown(f\"# SAM2 画像切り抜きツール（Ver{VERSION}）\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 1. 解析設定\")\n",
    "            model_drop = gr.Dropdown([\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"], label=\"モデル選択\", value=\"sam2_hiera_tiny.pt\")\n",
    "            preset = gr.Radio([\"標準\", \"詳細\", \"軽量\"], label=\"プリセット\", value=\"標準\")\n",
    "            reset_btn = gr.Button(\"全てリセット\", variant=\"stop\")\n",
    "            with gr.Accordion(\"パラメーター\", open=False):\n",
    "                p_side = gr.Slider(8, 128, 32, step=8, label=\"ポイント密度\"); iou_th = gr.Slider(0.0, 1.0, 0.8, label=\"IOUしきい値\"); stab_th = gr.Slider(0.0, 1.0, 0.92, label=\"安定しきい値\"); min_area = gr.Slider(0, 1000, 300, label=\"最小面積（ノイズ除去）\")\n",
    "            input_file = gr.File(label=\"画像をアップロード\")\n",
    "            input_preview = gr.Image(label=\"アップロード確認（プレビュー）\", type=\"pil\", interactive=False)\n",
    "            run_btn = gr.Button(\"解析実行\", variant=\"primary\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 2. 切り抜き作業\")\n",
    "            seg_view = gr.Image(label=\"解析画像（切り抜き部分をなぞる）\", tool=\"sketch\", type=\"pil\", interactive=True, elem_classes=\"padded-image\")\n",
    "            with gr.Accordion(\"マスク調整\", open=False):\n",
    "                inv = gr.Checkbox(label=\"反転\")\n",
    "                sm = gr.Slider(0, 20, 0, step=1, label=\"滑らか\")\n",
    "                ex = gr.Slider(-20, 20, 0, step=1, label=\"拡張\")\n",
    "            mask_btn = gr.Button(\"マスク生成\", variant=\"primary\")\n",
    "            mask_view = gr.Image(label=\"マスク生成結果（消しゴム編集可能）\", tool=\"sketch\", type=\"pil\", interactive=True, elem_classes=\"padded-image\")\n",
    "            \n",
    "            # デフォルトで閉じる設定 (open=False)\n",
    "            with gr.Accordion(\"切り抜き調整\", open=False):\n",
    "                final_inv_chk = gr.Checkbox(label=\"切り抜き時にマスクを反転\", value=False)\n",
    "                crop_blur = gr.Slider(0, 20, 0, step=1, label=\"ぼかし (px)\")\n",
    "            \n",
    "            crop_btn = gr.Button(\"切り抜き実行\", variant=\"primary\")\n",
    "            result_view = gr.Image(label=\"結果画像\", type=\"pil\", interactive=False)\n",
    "            result_file = gr.File(label=\"保存\")\n",
    "\n",
    "    gr.HTML(f\"<div class='version-text'>{version_info}</div>\")\n",
    "\n",
    "    def set_p(p): return {\"標準\":(32, 0.8, 0.92, 300), \"詳細\":(64, 0.5, 0.5, 100), \"軽量\":(16, 0.8, 0.95, 400)}[p]\n",
    "    preset.change(set_p, preset, [p_side, iou_th, stab_th, min_area])\n",
    "    input_file.change(handler.handle_upload, input_file, input_preview)\n",
    "    \n",
    "    run_btn.click(lambda: None, None, seg_view).then(\n",
    "        handler.run_segment, [model_drop, p_side, iou_th, stab_th, min_area], seg_view, show_progress=True\n",
    "    )\n",
    "    mask_btn.click(lambda: None, None, mask_view).then(\n",
    "        handler.make_mask, [seg_view, inv, sm, ex], mask_view, show_progress=True\n",
    "    )\n",
    "    crop_btn.click(\n",
    "        handler.run_crop, [mask_view, final_inv_chk, crop_blur], [result_view, result_file], show_progress=True\n",
    "    )\n",
    "    reset_btn.click(lambda: [None] * 6 + [False, 0], None, [input_file, input_preview, seg_view, mask_view, result_view, result_file, final_inv_chk, crop_blur])\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8aeda-42ce-4934-938d-21cb91be8883",
   "metadata": {},
   "source": [
    "## アプリケーションの起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05c2c6-a751-4c2a-94cf-7d6a3ea010da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notebook側の環境変数も Agg に固定して実行\n",
    "%env MPLBACKEND=Agg\n",
    "!{venv_path}/bin/python sac.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
